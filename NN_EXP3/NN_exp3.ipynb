{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4912, Test Accuracy: 92.50%\n",
      "Epoch 2, Loss: 0.2029, Test Accuracy: 94.13%\n",
      "Epoch 3, Loss: 0.1489, Test Accuracy: 94.90%\n",
      "Epoch 4, Loss: 0.1180, Test Accuracy: 95.38%\n",
      "Epoch 5, Loss: 0.1066, Test Accuracy: 95.31%\n",
      "Epoch 6, Loss: 0.0938, Test Accuracy: 95.41%\n",
      "Epoch 7, Loss: 0.0846, Test Accuracy: 95.65%\n",
      "Epoch 8, Loss: 0.0795, Test Accuracy: 96.11%\n",
      "Epoch 9, Loss: 0.0734, Test Accuracy: 95.72%\n",
      "Epoch 10, Loss: 0.0622, Test Accuracy: 96.26%\n",
      "Epoch 11, Loss: 0.0611, Test Accuracy: 95.49%\n",
      "Epoch 12, Loss: 0.0631, Test Accuracy: 96.11%\n",
      "Epoch 13, Loss: 0.0604, Test Accuracy: 95.91%\n",
      "Epoch 14, Loss: 0.0593, Test Accuracy: 95.52%\n",
      "Epoch 15, Loss: 0.0612, Test Accuracy: 96.22%\n",
      "Epoch 16, Loss: 0.0500, Test Accuracy: 96.23%\n",
      "Epoch 17, Loss: 0.0467, Test Accuracy: 95.74%\n",
      "Epoch 18, Loss: 0.0492, Test Accuracy: 96.65%\n",
      "Epoch 19, Loss: 0.0441, Test Accuracy: 96.37%\n",
      "Epoch 20, Loss: 0.0426, Test Accuracy: 96.52%\n",
      "\n",
      "Final Test Accuracy: 96.52% \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Disable eager execution (for TensorFlow v2 compatibility with v1 placeholders)\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = tfds.load('mnist', split=['train', 'test'], as_supervised=True)\n",
    "train_data, test_data = mnist\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess(dataset):\n",
    "    images, labels = [], []\n",
    "    for img, label in tfds.as_numpy(dataset):\n",
    "        images.append(img.flatten() / 255.0)  # Normalize pixel values (0-1)\n",
    "        labels.append(np.eye(10)[label])  # Convert to one-hot encoding\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Prepare train and test datasets\n",
    "train_images, train_labels = preprocess(train_data)\n",
    "test_images, test_labels = preprocess(test_data)\n",
    "\n",
    "# Define network parameters\n",
    "input_size = 784  # 28x28 pixels\n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 64\n",
    "output_size = 10  # 10 classes (digits 0-9)\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "# Define placeholders for input and output\n",
    "X = tf.compat.v1.placeholder(tf.float32, [None, input_size])\n",
    "Y = tf.compat.v1.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "# Initialize weights and biases\n",
    "weights = {\n",
    "    'w1': tf.Variable(tf.random.normal([input_size, hidden_size1])),\n",
    "    'w2': tf.Variable(tf.random.normal([hidden_size1, hidden_size2])),\n",
    "    'w3': tf.Variable(tf.random.normal([hidden_size2, output_size]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random.normal([hidden_size1])),\n",
    "    'b2': tf.Variable(tf.random.normal([hidden_size2])),\n",
    "    'b3': tf.Variable(tf.random.normal([output_size]))\n",
    "}\n",
    "\n",
    "# Define the feed-forward network\n",
    "def neural_network(x):\n",
    "    layer1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer1 = tf.nn.sigmoid(layer1)  # Using Sigmoid\n",
    "\n",
    "    layer2 = tf.add(tf.matmul(layer1, weights['w2']), biases['b2'])\n",
    "    layer2 = tf.nn.sigmoid(layer2)  # Using Sigmoid\n",
    "\n",
    "    output_layer = tf.add(tf.matmul(layer2, weights['w3']), biases['b3'])\n",
    "    return output_layer\n",
    "\n",
    "# Forward propagation\n",
    "logits = neural_network(X)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss function and optimizer (Backpropagation)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) * 100\n",
    "\n",
    "# Train the model\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_batches = len(train_images) // batch_size\n",
    "        avg_loss = 0\n",
    "\n",
    "        for i in range(total_batches):\n",
    "            batch_x = train_images[i * batch_size:(i + 1) * batch_size]\n",
    "            batch_y = train_labels[i * batch_size:(i + 1) * batch_size]\n",
    "            _, batch_loss = sess.run([optimizer, loss], feed_dict={X: batch_x, Y: batch_y})\n",
    "            avg_loss += batch_loss / total_batches\n",
    "\n",
    "        acc = sess.run(accuracy, feed_dict={X: test_images, Y: test_labels})\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Test Accuracy: {acc:.2f}%\")  # Show accuracy in %\n",
    "\n",
    "    # Final test accuracy in percentage\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: test_images, Y: test_labels})\n",
    "    print(f\"\\nFinal Test Accuracy: {test_acc:.2f}% \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
