DESCRIPTION OF MODEL ->
This model implements a single-layer perceptron for binary classification, trained using the Perceptron Learning Rule. It consists of a simple feedforward neural network with adjustable weights, including a bias term.

DESCRIPTION OF CODE  ->
# Model Initialization:
The Perceptron class is initialized with the following parameters:

-input_size: Number of input features.
=learning_rate: The step size for updating weights.
-epochs: Number of iterations for training.
The model also initializes random weights, including a bias term.

#Activation Function:
The perceptron uses a step activation function, which returns 1 if the input is greater than or equal to zero, otherwise returns 0. This function is used to make binary classifications.

#Prediction function:
The input is first modified to include a bias term.
The dot product of weights and input features is computed.
The activation function is applied to determine the final output (0 or 1).

#Training Function:
A bias term is added to the input features.
For each training sample, the perceptron calculates a prediction.
The weight update rule is applied here.
This process repeats for the given number of epochs.

MODEL PERFORMANCE EVALUATION->
- XOR Dataset:
The model is tested on logic gates with truth table inputs.
It correctly classifies NAND and other linearly separable functions.
However, a single-layer perceptron fails to learn XOR due to its non-linearity.
The final perceptron attempts to combine outputs from multiple perceptrons to improve classification accuracy.


